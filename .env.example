# AI Provider API Keys (configure at least one)

# OpenAI API Key
OPENAI_API_KEY=your-openai-api-key-here
# OpenAI API Base URL (optional, for using local LLM proxies like litellm)
# OPENAI_API_BASE=http://localhost:8080/v1

# Anthropic API Key  
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Groq API Key
GROQ_API_KEY=your-groq-api-key-here

# Cerebras API Key
CEREBRAS_API_KEY=your-cerebras-api-key-here

# Google Vertex AI Configuration
GOOGLE_VERTEX_PROJECT_ID=your-google-cloud-project-id
GOOGLE_VERTEX_REGION=us-central1

# GitHub API Token (optional, for GitHub integration)
GITHUB_TOKEN=your-github-token-here

# Arcade API Key (for web search functionality)
ARCADE_API_KEY=your-arcade-api-key-here

# Local Embeddings Configuration (optional)
# Embeddings are used for semantic search in project notes and files
# Provider options: auto (try local, fallback to OpenAI) | local | openai
EMBEDDINGS_PROVIDER=auto
# Device for local embeddings: auto (default) | cpu | cuda | mps
EMBEDDINGS_DEVICE=auto
# Custom Python path (optional, defaults to python3)
# EMBEDDINGS_PYTHON=/path/to/python3
# Disable OpenAI fallback when local embeddings fail
# EMBEDDINGS_FALLBACK_OPENAI=false

# Docker Environment Configuration

# Model Selection
OPENAI_MODEL=gpt-4-turbo-preview
CLAUDE_MODEL=claude-3-opus-20240229
OPENAI_ORG_ID=your-openai-org-id-here

# HuggingFace Configuration (for open-source models)
HUGGINGFACE_TOKEN=your-huggingface-token-here

# Aider Configuration
AIDER_MODEL=gpt-4-turbo-preview
AIDER_AUTO_COMMITS=false
AIDER_DARK_MODE=true

# OpenCode Configuration (Open-source models)
OLLAMA_HOST=http://localhost:11434
OPENCODE_MODEL=codellama
LITELLM_MODEL=

# API Proxy Configuration
PROXY_PORT=8080