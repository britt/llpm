# Docker Environment Configuration
# Copy this file to .env in the docker directory and fill in your API keys

# LiteLLM Proxy Configuration
# This is the master key used by all agents to authenticate with LiteLLM proxy
LITELLM_MASTER_KEY=sk-1234  # Change this to a secure key!
DATABASE_URL=  # Optional: SQLite/PostgreSQL URL for usage tracking

# AI Provider API Keys (configure at least one)
# These are used by the LiteLLM proxy to connect to actual AI providers

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_ORG_ID=your-openai-org-id-here

# Anthropic Configuration  
ANTHROPIC_API_KEY=your-anthropic-api-key-here
CLAUDE_MODEL=claude-3-opus-20240229

# Groq Configuration
GROQ_API_KEY=your-groq-api-key-here

# Google Vertex AI Configuration
GOOGLE_VERTEX_PROJECT_ID=your-google-cloud-project-id
GOOGLE_VERTEX_REGION=us-central1

# HuggingFace Configuration (for open-source models)
HUGGINGFACE_TOKEN=your-huggingface-token-here

# Aider Configuration
AIDER_MODEL=gpt-4-turbo-preview
AIDER_AUTO_COMMITS=false
AIDER_DARK_MODE=true
AIDER_CLI_OPTIONS=--no-auto-commits --dark-mode

# OpenCode Configuration (Open-source models)
OLLAMA_HOST=http://localhost:11434
OPENCODE_MODEL=codellama
LITELLM_MODEL=
OLLAMA_CLI_OPTIONS=
LITELLM_CLI_OPTIONS=

# CLI Options for Agents
# These options are automatically added when running the respective CLI tools
CLAUDE_CLI_OPTIONS=
OPENAI_CLI_OPTIONS=--skip-git-repo-check

# OAuth Callback Ports
# OpenAI Codex uses OAuth with a hardcoded container port (1455)
# When running multiple instances, map different host ports to avoid conflicts
# Default: 1455 (maps to container port 1455)
# For multiple instances, use: 1455, 1456, 1457, etc.
OPENAI_OAUTH_PORT=1455

# Workspace Path
# The directory on the host system that agents will mount as their workspace
# Default: ../workspace (relative to docker directory)
# You can use absolute paths or relative paths
# Example: WORKSPACE_PATH=/path/to/your/projects
WORKSPACE_PATH=../workspace

# Note: Never commit your actual .env file with real API keys!
# The docker/.env file is already in .gitignore

# How the LiteLLM Proxy Works:
# 1. All agents connect to http://litellm-proxy:4000 instead of direct AI providers
# 2. Agents use LITELLM_MASTER_KEY for authentication
# 3. LiteLLM proxy routes requests to appropriate providers using the API keys above
# 4. You can change models/providers centrally in litellm-proxy/litellm_config.yaml