services:
  # REST API Broker - HTTP interface for agents (singleton - not scaled)
  rest-broker:
    build:
      context: ./rest-broker
      dockerfile: Dockerfile
    image: llpm-rest-broker:latest
    container_name: rest-broker
    env_file:
      - .env
    environment:
      - PORT=3010
      - HOST=0.0.0.0
      - NODE_ENV=production
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - CORS_ORIGIN=${CORS_ORIGIN:-*}
      - RATE_LIMIT_WINDOW=60000
      - RATE_LIMIT_MAX=100
      - HEALTH_CHECK_INTERVAL=30000
      - USE_UNIX_SOCKETS=${USE_UNIX_SOCKETS:-false}
      - LITELLM_PROXY_URL=http://litellm-proxy:4000
      - LITELLM_BASE_URL=http://litellm-proxy:4000
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - AUTH_ENABLED=${AUTH_ENABLED:-false}
      - AUTH_TOKEN=${AUTH_TOKEN}
      - AGENT_AUTH_TYPE=${AGENT_AUTH_TYPE:-api_key}
      - CLAUDE_MODEL=${CLAUDE_MODEL:-claude-3-5-sonnet-20241022}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4-turbo-preview}
    ports:
      - "3010:3010"
    networks:
      - llpm-network
    depends_on:
      - litellm-proxy
      - claude-code
      - openai-codex
      - aider
      - opencode
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  # Docker socket for container management
      - /var/run/llpm:/var/run/llpm  # Unix socket directory (if used)
      - ${WORKSPACE_PATH:-./workspace}:/workspace:rw
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3010/health', (r) => r.statusCode === 200 ? process.exit(0) : process.exit(1))"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # PostgreSQL Database for LiteLLM
  postgres:
    image: postgres:15-alpine
    container_name: litellm-postgres
    environment:
      - POSTGRES_USER=litellm
      - POSTGRES_PASSWORD=litellm123
      - POSTGRES_DB=litellm
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - llpm-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U litellm"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # LiteLLM Proxy - Central AI Gateway (singleton - not scaled)
  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-proxy  # Keep container_name for singleton services
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_ORG_ID=${OPENAI_ORG_ID}
      - OPENAI_ORGANIZATION=${OPENAI_ORG_ID}  # OpenAI SDK expects this exact name
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - GOOGLE_VERTEX_PROJECT_ID=${GOOGLE_VERTEX_PROJECT_ID}
      - GOOGLE_VERTEX_REGION=${GOOGLE_VERTEX_REGION:-us-central1}
      - OLLAMA_API_BASE=http://opencode:11434
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - DATABASE_URL=postgresql://litellm:litellm123@postgres:5432/litellm
      - STORE_MODEL_IN_DB=True
    volumes:
      - ./litellm-proxy/proxy_config.yaml:/app/config.yaml:ro
      - ./litellm-proxy/custom_auth.py:/app/custom_auth.py:ro
    command: ["--config", "/app/config.yaml", "--port", "4000", "--detailed_debug"]
    ports:
      - "4000:4000"
    networks:
      - llpm-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # Base image builder (singleton - not scaled)
  base:
    build:
      context: ./base
      dockerfile: Dockerfile
    image: llpm-base:latest
    container_name: llpm-base-builder  # Keep container_name for singleton services
    command: /bin/true

  # Claude Code Assistant (scalable)
  claude-code:
    build:
      context: .
      dockerfile: ./claude-code/Dockerfile
    depends_on:
      - base
      - litellm-proxy
    image: llpm-claude-code:latest
    # No container_name to allow scaling
    env_file:
      - .env
    environment:
      # Use LiteLLM proxy for Anthropic API
      - ANTHROPIC_BASE_URL=${ANTHROPIC_BASE_URL:-http://litellm-proxy:4000}
      # In subscription mode, ANTHROPIC_API_KEY should be empty so Claude uses OAuth credentials
      # In api_key mode, ANTHROPIC_API_KEY is set in the environment
      # Note: To run in subscription mode, set ANTHROPIC_API_KEY="" in your shell before docker-compose up
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - CLAUDE_MODEL=${CLAUDE_MODEL:-}
      - CLAUDE_CLI_OPTIONS=${CLAUDE_CLI_OPTIONS:-}
      # Agent registration configuration
      - AGENT_AUTH_TYPE=${AGENT_AUTH_TYPE:-api_key}
      - AGENT_PROVIDER=claude
      - AGENT_MODEL=${CLAUDE_MODEL:-claude-sonnet-4-5}
    volumes:
      - ${WORKSPACE_PATH:-../workspace}:/home/claude/workspace
      - ${HOME}/.ssh:/home/claude/.ssh:ro
      - ${HOME}/.gitconfig:/home/claude/.gitconfig:ro
    stdin_open: true
    tty: true
    networks:
      - llpm-network
    healthcheck:
      test: ["CMD", "/usr/local/bin/healthcheck.sh"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s

  # OpenAI Codex Assistant (scalable)
  openai-codex:
    build:
      context: .
      dockerfile: ./openai-codex/Dockerfile
    depends_on:
      - base
      - litellm-proxy
    image: llpm-openai-codex:latest
    # No container_name to allow scaling
    env_file:
      - .env
    environment:
      # Use LiteLLM proxy for OpenAI API
      # In subscription mode, OPENAI_API_BASE should be http://litellm-proxy:4000/codex for passthrough
      # In api_key mode, use http://litellm-proxy:4000 without passthrough
      # Note: To run in subscription mode, set OPENAI_API_KEY="" and OPENAI_API_BASE="http://litellm-proxy:4000/codex" in your shell
      - OPENAI_API_BASE=${OPENAI_API_BASE:-http://litellm-proxy:4000}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4-turbo-preview}
      - OPENAI_ORG_ID=${OPENAI_ORG_ID}
      - OPENAI_CLI_OPTIONS=${OPENAI_CLI_OPTIONS:-}
      # Agent registration configuration
      - AGENT_AUTH_TYPE=${AGENT_AUTH_TYPE:-api_key}
      - AGENT_PROVIDER=openai
      - AGENT_MODEL=${OPENAI_MODEL:-gpt-4-turbo-preview}
    ports:
      # OAuth callback port for authentication
      # Container port 1455 is hardcoded by OpenAI CLI
      # Map to host port via OPENAI_OAUTH_PORT (default: 1455)
      # For multiple instances, use different ports: OPENAI_OAUTH_PORT=1456, 1457, etc.
      - "${OPENAI_OAUTH_PORT:-1455}:1455"
    volumes:
      - ${WORKSPACE_PATH:-../workspace}:/codex-workspace
      - ${HOME}/.ssh:/root/.ssh:ro
      - ${HOME}/.gitconfig:/root/.gitconfig:ro
    working_dir: /codex-workspace
    stdin_open: true
    tty: true
    networks:
      - llpm-network
    healthcheck:
      test: ["CMD", "/usr/local/bin/healthcheck.sh"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s

  # Aider Assistant (scalable)
  aider:
    build:
      context: .
      dockerfile: ./aider/Dockerfile
    depends_on:
      - base
      - litellm-proxy
    image: llpm-aider:latest
    # No container_name to allow scaling
    env_file:
      - .env
    environment:
      # Use LiteLLM proxy for all APIs
      - OPENAI_API_BASE=http://litellm-proxy:4000
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - AIDER_OPENAI_API_BASE=http://litellm-proxy:4000
      - ANTHROPIC_BASE_URL=http://litellm-proxy:4000
      - ANTHROPIC_API_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - AIDER_MODEL=${AIDER_MODEL:-gpt-4-turbo-preview}
      - AIDER_AUTO_COMMITS=${AIDER_AUTO_COMMITS:-false}
      - AIDER_DARK_MODE=${AIDER_DARK_MODE:-true}
      - AIDER_CLI_OPTIONS=${AIDER_CLI_OPTIONS:-}
    volumes:
      - ${WORKSPACE_PATH:-../workspace}:/aider-workspace
      - ${HOME}/.ssh:/root/.ssh:ro
      - ${HOME}/.gitconfig:/root/.gitconfig:ro
    working_dir: /aider-workspace
    stdin_open: true
    tty: true
    networks:
      - llpm-network
    healthcheck:
      test: ["CMD", "bash", "-c", "/usr/local/bin/healthcheck.sh && which aider"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s

  # OpenCode Assistant (scalable)
  opencode:
    build:
      context: .
      dockerfile: ./opencode/Dockerfile
    depends_on:
      - base
    image: llpm-opencode:latest
    # No container_name to allow scaling
    env_file:
      - .env
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://localhost:11434}
      - OPENCODE_MODEL=${OPENCODE_MODEL:-codellama}
      # OpenCode can use its own LiteLLM or connect to proxy
      - LITELLM_PROXY_BASE=http://litellm-proxy:4000
      - LITELLM_PROXY_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - OLLAMA_CLI_OPTIONS=${OLLAMA_CLI_OPTIONS:-}
      - LITELLM_CLI_OPTIONS=${LITELLM_CLI_OPTIONS:-}
    volumes:
      - ${WORKSPACE_PATH:-../workspace}:/opencode-workspace
      - ${HOME}/.ssh:/root/.ssh:ro
      - ${HOME}/.gitconfig:/root/.gitconfig:ro
      - ollama-data:/root/.ollama
    working_dir: /opencode-workspace
    stdin_open: true
    tty: true
    networks:
      - llpm-network
    healthcheck:
      test: ["CMD", "/usr/local/bin/healthcheck.sh"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s


networks:
  llpm-network:
    driver: bridge
    name: llpm-network

volumes:
  ollama-data:
    driver: local
  postgres-data:
    driver: local