# Context-Aware Question Generator Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build a skill with AI tools that proactively identifies information gaps in project context and generates prioritized, actionable questions to surface missing requirements, ambiguities, and inconsistencies.

**Architecture:** LLM-driven gap analysis. Tools gather context from multiple sources (issues, PRs, notes, project scan), then use the LLM to analyze for gaps and generate questions. Each question references its source and is categorized/prioritized. The skill orchestrates when to use which tool based on natural language triggers.

**Tech Stack:** TypeScript, Zod schemas, Vitest, existing services (GitHub, notes, project scan), AI SDK for LLM analysis

---

## Phase 1: Types and Interfaces

### Task 1.1: Define Question Types

**Files:**
- Create: `src/types/questions.ts`
- Test: `src/types/questions.test.ts`

**Step 1: Write the failing test**

```typescript
// src/types/questions.test.ts
import { describe, it, expect } from 'vitest';
import {
  type Question,
  type QuestionCategory,
  type QuestionPriority,
  type QuestionSource,
  type ProjectQuestionsResult,
  type IssueQuestionsResult,
  isQuestion,
  createQuestion,
} from './questions';

describe('questions types', () => {
  describe('QuestionCategory', () => {
    it('should include all expected categories', () => {
      const categories: QuestionCategory[] = [
        'requirements',
        'technical',
        'documentation',
        'process',
        'consistency',
        'architecture',
      ];
      expect(categories).toHaveLength(6);
    });
  });

  describe('QuestionPriority', () => {
    it('should include all expected priorities', () => {
      const priorities: QuestionPriority[] = ['high', 'medium', 'low'];
      expect(priorities).toHaveLength(3);
    });
  });

  describe('isQuestion', () => {
    it('should return false for null', () => {
      expect(isQuestion(null)).toBe(false);
    });

    it('should return false for object missing required fields', () => {
      expect(isQuestion({ id: 'q1' })).toBe(false);
    });

    it('should return true for valid Question', () => {
      const question = createQuestion({
        category: 'requirements',
        priority: 'high',
        question: 'What are the acceptance criteria?',
        context: 'Issue #42 has no acceptance criteria defined',
        source: {
          type: 'issue',
          reference: '#42',
          url: 'https://github.com/owner/repo/issues/42',
        },
      });
      expect(isQuestion(question)).toBe(true);
    });
  });

  describe('createQuestion', () => {
    it('should generate a unique id', () => {
      const q1 = createQuestion({
        category: 'technical',
        priority: 'medium',
        question: 'Question 1',
        context: 'Context 1',
        source: { type: 'file', reference: 'src/index.ts' },
      });
      const q2 = createQuestion({
        category: 'technical',
        priority: 'medium',
        question: 'Question 2',
        context: 'Context 2',
        source: { type: 'file', reference: 'src/index.ts' },
      });
      expect(q1.id).not.toBe(q2.id);
    });

    it('should set all required fields', () => {
      const question = createQuestion({
        category: 'documentation',
        priority: 'low',
        question: 'Should we add JSDoc?',
        context: 'No inline documentation found',
        source: { type: 'project-scan', reference: 'project.json#documentation' },
        suggestedAction: 'Add JSDoc comments to exported functions',
      });

      expect(question.category).toBe('documentation');
      expect(question.priority).toBe('low');
      expect(question.question).toBe('Should we add JSDoc?');
      expect(question.context).toBe('No inline documentation found');
      expect(question.source.type).toBe('project-scan');
      expect(question.suggestedAction).toBe('Add JSDoc comments to exported functions');
    });
  });
});
```

**Step 2: Run test to verify it fails**

Run: `bun run test src/types/questions.test.ts`
Expected: FAIL with "Cannot find module './questions'"

**Step 3: Write minimal implementation**

```typescript
// src/types/questions.ts
/**
 * Types for the Context-Aware Question Generator feature
 *
 * These types define the structure of questions generated by analyzing
 * project context for information gaps.
 */

/**
 * Categories of questions that can be generated
 */
export type QuestionCategory =
  | 'requirements'    // Missing acceptance criteria, vague descriptions
  | 'technical'       // Undocumented code, unclear error handling
  | 'documentation'   // Missing docs, outdated README
  | 'process'         // Stale issues/PRs, missing assignees
  | 'consistency'     // Contradictory information across sources
  | 'architecture';   // Component relationships, dependency issues

/**
 * Priority levels for questions
 */
export type QuestionPriority = 'high' | 'medium' | 'low';

/**
 * Types of sources that questions can reference
 */
export type QuestionSourceType =
  | 'issue'
  | 'pr'
  | 'file'
  | 'note'
  | 'readme'
  | 'project-scan'
  | 'architecture';

/**
 * Reference to the source of a question
 */
export interface QuestionSource {
  type: QuestionSourceType;
  reference: string;       // e.g., "#45", "src/auth/index.ts", "note-123"
  url?: string;           // Optional URL for GitHub items
}

/**
 * A single question generated from context analysis
 */
export interface Question {
  id: string;
  category: QuestionCategory;
  priority: QuestionPriority;
  question: string;
  context: string;           // Why this question matters
  source: QuestionSource;
  suggestedAction?: string;  // e.g., "Add acceptance criteria to issue #45"
  requiresScan?: boolean;    // True if this question type requires project scan
}

/**
 * Input for creating a question (without id)
 */
export type QuestionInput = Omit<Question, 'id'>;

/**
 * Type guard to check if an object is a valid Question
 */
export function isQuestion(obj: unknown): obj is Question {
  if (obj === null || typeof obj !== 'object') {
    return false;
  }

  const q = obj as Record<string, unknown>;

  return (
    typeof q.id === 'string' &&
    typeof q.category === 'string' &&
    typeof q.priority === 'string' &&
    typeof q.question === 'string' &&
    typeof q.context === 'string' &&
    typeof q.source === 'object' &&
    q.source !== null &&
    typeof (q.source as Record<string, unknown>).type === 'string' &&
    typeof (q.source as Record<string, unknown>).reference === 'string'
  );
}

/**
 * Counter for generating unique question IDs
 */
let questionIdCounter = 0;

/**
 * Create a question with a unique ID
 */
export function createQuestion(input: QuestionInput): Question {
  questionIdCounter++;
  return {
    id: `q-${Date.now()}-${questionIdCounter}`,
    ...input,
  };
}

/**
 * Summary of sources analyzed during question generation
 */
export interface SourcesAnalyzed {
  projectScan: boolean;
  readme: boolean;
  codebase: { files: number; modules: number };
  issues: { open: number; analyzed: number };
  prs: { open: number; analyzed: number };
  notes: { total: number; analyzed: number };
}

/**
 * Summary statistics for generated questions
 */
export interface QuestionsSummary {
  totalQuestions: number;
  byCategory: Record<QuestionCategory, number>;
  byPriority: Record<QuestionPriority, number>;
}

/**
 * Result from generate_project_questions tool
 */
export interface ProjectQuestionsResult {
  success: boolean;
  project: string;
  scannedAt: string;
  projectScanAvailable: boolean;
  sourcesAnalyzed: SourcesAnalyzed;
  questions: Question[];
  summary: QuestionsSummary;
  error?: string;
}

/**
 * Result from generate_issue_questions tool
 */
export interface IssueQuestionsResult {
  success: boolean;
  issueNumber: number;
  issueTitle: string;
  questions: Question[];
  summary: {
    totalQuestions: number;
    byCategory: Record<string, number>;
  };
  error?: string;
}

/**
 * Result from identify_information_gaps tool
 */
export interface InformationGapsResult {
  success: boolean;
  target: string;  // What was analyzed (e.g., "README", "documentation", "codebase")
  gaps: Question[];
  summary: {
    totalGaps: number;
    byCategory: Record<string, number>;
  };
  error?: string;
}

/**
 * Result from suggest_clarifications tool
 */
export interface ClarificationsResult {
  success: boolean;
  draftType: 'issue' | 'pr' | 'note' | 'document';
  clarifications: Question[];
  summary: {
    totalClarifications: number;
    byCategory: Record<string, number>;
  };
  error?: string;
}
```

**Step 4: Run test to verify it passes**

Run: `bun run test src/types/questions.test.ts`
Expected: PASS

**Step 5: Commit**

```bash
git add src/types/questions.ts src/types/questions.test.ts
git commit -m "feat(questions): add types for context-aware question generator

RED: Wrote tests for Question types, type guards, and factory function
GREEN: Implemented QuestionCategory, QuestionPriority, QuestionSource,
Question interface, isQuestion guard, and createQuestion factory

Part of issue #28"
```

---

## Phase 2: Gap Analysis Service

### Task 2.1: Create Gap Analyzer Service Interface

**Files:**
- Create: `src/services/gapAnalyzer.ts`
- Test: `src/services/gapAnalyzer.test.ts`

**Step 1: Write the failing test**

```typescript
// src/services/gapAnalyzer.test.ts
import { describe, it, expect, vi, beforeEach } from 'vitest';
import { GapAnalyzer } from './gapAnalyzer';
import type { Question } from '../types/questions';

// Mock dependencies
vi.mock('../utils/projectConfig', () => ({
  getCurrentProject: vi.fn(),
}));

vi.mock('./projectScanBackend', () => ({
  loadProjectScan: vi.fn(),
}));

vi.mock('./github', () => ({
  listIssues: vi.fn(),
  getIssueWithComments: vi.fn(),
}));

vi.mock('./notesBackend', () => ({
  getNotesBackend: vi.fn(),
}));

describe('GapAnalyzer', () => {
  let analyzer: GapAnalyzer;

  beforeEach(() => {
    vi.clearAllMocks();
    analyzer = new GapAnalyzer();
  });

  describe('constructor', () => {
    it('should create an instance', () => {
      expect(analyzer).toBeInstanceOf(GapAnalyzer);
    });
  });

  describe('analyzeIssue', () => {
    it('should return questions for issue with missing acceptance criteria', async () => {
      const mockIssue = {
        number: 42,
        title: 'Add feature X',
        body: 'We should add feature X.',
        state: 'open',
        labels: [],
        user: { login: 'author' },
        html_url: 'https://github.com/owner/repo/issues/42',
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-01T00:00:00Z',
      };

      const result = await analyzer.analyzeIssue(mockIssue, []);

      expect(result.questions.length).toBeGreaterThan(0);
      expect(result.questions.some(q => q.category === 'requirements')).toBe(true);
    });

    it('should detect missing labels', async () => {
      const mockIssue = {
        number: 43,
        title: 'Bug: Something is broken',
        body: '## Description\nSomething is broken.\n\n## Acceptance Criteria\n- [ ] It should work',
        state: 'open',
        labels: [],
        user: { login: 'author' },
        html_url: 'https://github.com/owner/repo/issues/43',
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-01T00:00:00Z',
      };

      const result = await analyzer.analyzeIssue(mockIssue, []);

      expect(result.questions.some(q =>
        q.question.toLowerCase().includes('label')
      )).toBe(true);
    });
  });

  describe('checkAcceptanceCriteria', () => {
    it('should return true when acceptance criteria present', () => {
      const body = '## Description\nDo X\n\n## Acceptance Criteria\n- [ ] Thing works';
      expect(analyzer.checkAcceptanceCriteria(body)).toBe(true);
    });

    it('should return false when acceptance criteria missing', () => {
      const body = 'Just do the thing please';
      expect(analyzer.checkAcceptanceCriteria(body)).toBe(false);
    });

    it('should detect alternative acceptance criteria formats', () => {
      expect(analyzer.checkAcceptanceCriteria('**AC:**\n- works')).toBe(true);
      expect(analyzer.checkAcceptanceCriteria('Done when:\n- works')).toBe(true);
      expect(analyzer.checkAcceptanceCriteria('Definition of Done\n- works')).toBe(true);
    });
  });

  describe('checkVagueDescription', () => {
    it('should flag very short descriptions', () => {
      expect(analyzer.checkVagueDescription('Fix it')).toBe(true);
      expect(analyzer.checkVagueDescription('Bug')).toBe(true);
    });

    it('should not flag detailed descriptions', () => {
      const detailed = `
        ## Problem
        When the user clicks the submit button, the form data is not being validated
        before sending to the server. This causes 500 errors when required fields are missing.

        ## Expected Behavior
        The form should validate all required fields client-side before submission.
      `;
      expect(analyzer.checkVagueDescription(detailed)).toBe(false);
    });
  });
});
```

**Step 2: Run test to verify it fails**

Run: `bun run test src/services/gapAnalyzer.test.ts`
Expected: FAIL with "Cannot find module './gapAnalyzer'"

**Step 3: Write minimal implementation**

```typescript
// src/services/gapAnalyzer.ts
/**
 * GapAnalyzer Service
 *
 * Analyzes project context (issues, PRs, notes, codebase) to identify
 * information gaps and generate clarifying questions.
 */
import type {
  Question,
  QuestionCategory,
  QuestionPriority,
  QuestionInput,
} from '../types/questions';
import { createQuestion } from '../types/questions';
import { debug } from '../utils/logger';

/**
 * Simplified GitHub issue shape for analysis
 */
interface AnalyzableIssue {
  number: number;
  title: string;
  body: string | null;
  state: string;
  labels: Array<{ name: string }>;
  user: { login: string };
  html_url: string;
  created_at: string;
  updated_at: string;
  assignee?: { login: string } | null;
}

/**
 * GitHub comment shape
 */
interface AnalyzableComment {
  id: number;
  body: string;
  user: { login: string };
  created_at: string;
}

/**
 * Result from analyzing a single issue
 */
export interface IssueAnalysisResult {
  issueNumber: number;
  issueTitle: string;
  questions: Question[];
}

/**
 * Patterns that indicate acceptance criteria
 */
const ACCEPTANCE_CRITERIA_PATTERNS = [
  /acceptance\s*criteria/i,
  /\bac\b\s*:/i,
  /definition\s*of\s*done/i,
  /done\s*when/i,
  /success\s*criteria/i,
  /##\s*criteria/i,
  /##\s*requirements/i,
  /\[\s*[x ]\s*\]/i,  // Checkbox format often indicates criteria
];

/**
 * Minimum description length (characters) to not be considered vague
 */
const MIN_DESCRIPTION_LENGTH = 100;

/**
 * Words that suggest vague descriptions when used alone
 */
const VAGUE_INDICATORS = [
  'fix', 'bug', 'broken', 'issue', 'problem', 'error',
  'not working', 'doesnt work', "doesn't work", 'help',
];

export class GapAnalyzer {
  constructor() {
    debug('GapAnalyzer initialized');
  }

  /**
   * Analyze a GitHub issue for information gaps
   */
  async analyzeIssue(
    issue: AnalyzableIssue,
    comments: AnalyzableComment[]
  ): Promise<IssueAnalysisResult> {
    const questions: Question[] = [];
    const issueUrl = issue.html_url;
    const issueRef = `#${issue.number}`;

    // Check for missing acceptance criteria
    if (!this.checkAcceptanceCriteria(issue.body || '')) {
      questions.push(createQuestion({
        category: 'requirements',
        priority: 'high',
        question: `What are the acceptance criteria for issue ${issueRef}?`,
        context: `Issue "${issue.title}" lacks clear acceptance criteria, making it difficult to determine when the work is complete.`,
        source: { type: 'issue', reference: issueRef, url: issueUrl },
        suggestedAction: `Add an "Acceptance Criteria" section to issue ${issueRef}`,
      }));
    }

    // Check for vague description
    if (this.checkVagueDescription(issue.body || '')) {
      questions.push(createQuestion({
        category: 'requirements',
        priority: 'medium',
        question: `Can you provide more details about issue ${issueRef}?`,
        context: `The description for "${issue.title}" is brief and may not provide enough context for implementation.`,
        source: { type: 'issue', reference: issueRef, url: issueUrl },
        suggestedAction: `Expand the description with problem details, expected behavior, and context`,
      }));
    }

    // Check for missing labels
    if (issue.labels.length === 0) {
      questions.push(createQuestion({
        category: 'process',
        priority: 'low',
        question: `What labels should be applied to issue ${issueRef}?`,
        context: `Issue "${issue.title}" has no labels, making it harder to categorize and prioritize.`,
        source: { type: 'issue', reference: issueRef, url: issueUrl },
        suggestedAction: `Add appropriate labels (e.g., bug, enhancement, priority)`,
      }));
    }

    // Check for missing assignee on open issues
    if (issue.state === 'open' && !issue.assignee) {
      questions.push(createQuestion({
        category: 'process',
        priority: 'low',
        question: `Who should be assigned to issue ${issueRef}?`,
        context: `Open issue "${issue.title}" has no assignee.`,
        source: { type: 'issue', reference: issueRef, url: issueUrl },
        suggestedAction: `Assign someone to own this issue`,
      }));
    }

    // Check for stale issues (no updates in 30+ days)
    const lastUpdated = new Date(issue.updated_at);
    const thirtyDaysAgo = new Date();
    thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30);

    if (issue.state === 'open' && lastUpdated < thirtyDaysAgo) {
      questions.push(createQuestion({
        category: 'process',
        priority: 'medium',
        question: `Is issue ${issueRef} still relevant?`,
        context: `Issue "${issue.title}" hasn't been updated in over 30 days.`,
        source: { type: 'issue', reference: issueRef, url: issueUrl },
        suggestedAction: `Review and update status, or close if no longer needed`,
      }));
    }

    return {
      issueNumber: issue.number,
      issueTitle: issue.title,
      questions,
    };
  }

  /**
   * Check if issue body contains acceptance criteria
   */
  checkAcceptanceCriteria(body: string): boolean {
    if (!body || body.trim().length === 0) {
      return false;
    }

    return ACCEPTANCE_CRITERIA_PATTERNS.some(pattern => pattern.test(body));
  }

  /**
   * Check if description is too vague
   */
  checkVagueDescription(body: string): boolean {
    if (!body) {
      return true;
    }

    const trimmed = body.trim();

    // Very short descriptions are vague
    if (trimmed.length < MIN_DESCRIPTION_LENGTH) {
      // But allow short descriptions with checkboxes (likely well-structured)
      if (/\[\s*[x ]\s*\]/.test(trimmed)) {
        return false;
      }
      return true;
    }

    // Check if description is mostly vague indicators
    const lowerBody = trimmed.toLowerCase();
    const vagueCount = VAGUE_INDICATORS.filter(v => lowerBody.includes(v)).length;

    // If more than half the words are vague indicators and it's short
    if (vagueCount >= 3 && trimmed.length < 200) {
      return true;
    }

    return false;
  }

  /**
   * Prioritize questions based on impact
   */
  prioritizeQuestions(questions: Question[]): Question[] {
    const priorityOrder: Record<QuestionPriority, number> = {
      high: 0,
      medium: 1,
      low: 2,
    };

    return [...questions].sort((a, b) => {
      const priorityDiff = priorityOrder[a.priority] - priorityOrder[b.priority];
      if (priorityDiff !== 0) return priorityDiff;

      // Secondary sort by category (requirements first)
      const categoryOrder: Record<QuestionCategory, number> = {
        requirements: 0,
        technical: 1,
        architecture: 2,
        consistency: 3,
        documentation: 4,
        process: 5,
      };
      return categoryOrder[a.category] - categoryOrder[b.category];
    });
  }
}
```

**Step 4: Run test to verify it passes**

Run: `bun run test src/services/gapAnalyzer.test.ts`
Expected: PASS

**Step 5: Commit**

```bash
git add src/services/gapAnalyzer.ts src/services/gapAnalyzer.test.ts
git commit -m "feat(gapAnalyzer): add service for issue gap analysis

RED: Wrote tests for GapAnalyzer with issue analysis and gap detection
GREEN: Implemented analyzeIssue, checkAcceptanceCriteria, checkVagueDescription

Part of issue #28"
```

---

### Task 2.2: Add Project-Level Gap Analysis

**Files:**
- Modify: `src/services/gapAnalyzer.ts`
- Test: `src/services/gapAnalyzer.test.ts`

**Step 1: Write the failing test**

```typescript
// Add to src/services/gapAnalyzer.test.ts

describe('analyzeProjectContext', () => {
  it('should suggest running project scan when not available', async () => {
    const { getCurrentProject } = await import('../utils/projectConfig');
    const { loadProjectScan } = await import('./projectScanBackend');

    vi.mocked(getCurrentProject).mockResolvedValue({
      id: 'test-project',
      name: 'Test Project',
      path: '/path/to/project',
      github_repo: 'owner/repo',
    });
    vi.mocked(loadProjectScan).mockResolvedValue(null);

    const result = await analyzer.analyzeProjectContext();

    expect(result.projectScanAvailable).toBe(false);
    expect(result.questions.some(q =>
      q.question.toLowerCase().includes('project scan') ||
      q.suggestedAction?.toLowerCase().includes('scan')
    )).toBe(true);
  });

  it('should analyze architecture when project scan available', async () => {
    const { getCurrentProject } = await import('../utils/projectConfig');
    const { loadProjectScan } = await import('./projectScanBackend');

    vi.mocked(getCurrentProject).mockResolvedValue({
      id: 'test-project',
      name: 'Test Project',
      path: '/path/to/project',
      github_repo: 'owner/repo',
    });
    vi.mocked(loadProjectScan).mockResolvedValue({
      version: '1.0.0',
      scannedAt: '2025-01-01T00:00:00Z',
      projectId: 'test-project',
      projectName: 'Test Project',
      projectPath: '/path/to/project',
      overview: {
        summary: '',
        primaryLanguages: ['TypeScript'],
        frameworks: [],
        projectType: 'cli',
        totalFiles: 100,
        totalLines: 5000,
        totalSize: 250000,
      },
      directoryStructure: [],
      keyFiles: [
        { path: 'src/index.ts', reason: 'Entry point', category: 'entry-point' },
      ],
      documentation: {
        hasDocumentation: false,
        docFiles: [],
        inlineDocsCoverage: 'none',
      },
      dependencies: {
        packageManager: 'bun',
        runtime: [],
        development: [],
      },
      architecture: {
        description: '',
        components: [],
      },
    });

    const result = await analyzer.analyzeProjectContext();

    expect(result.projectScanAvailable).toBe(true);
    // Should generate questions about missing docs
    expect(result.questions.some(q => q.category === 'documentation')).toBe(true);
  });
});
```

**Step 2: Run test to verify it fails**

Run: `bun run test src/services/gapAnalyzer.test.ts`
Expected: FAIL with "analyzer.analyzeProjectContext is not a function"

**Step 3: Add implementation**

```typescript
// Add to src/services/gapAnalyzer.ts

import { getCurrentProject } from '../utils/projectConfig';
import { loadProjectScan } from './projectScanBackend';
import type { ProjectScan } from '../types/projectScan';

/**
 * Result from analyzing project context
 */
export interface ProjectContextAnalysisResult {
  projectName: string;
  projectScanAvailable: boolean;
  scannedAt?: string;
  sourcesAnalyzed: {
    projectScan: boolean;
    readme: boolean;
    issues: { open: number; analyzed: number };
    notes: { total: number; analyzed: number };
  };
  questions: Question[];
}

// Add this method to the GapAnalyzer class:

  /**
   * Analyze overall project context for gaps
   */
  async analyzeProjectContext(): Promise<ProjectContextAnalysisResult> {
    const project = await getCurrentProject();

    if (!project) {
      return {
        projectName: 'Unknown',
        projectScanAvailable: false,
        sourcesAnalyzed: {
          projectScan: false,
          readme: false,
          issues: { open: 0, analyzed: 0 },
          notes: { total: 0, analyzed: 0 },
        },
        questions: [createQuestion({
          category: 'process',
          priority: 'high',
          question: 'No active project is set. Which project would you like to analyze?',
          context: 'A project must be set to analyze context.',
          source: { type: 'project-scan', reference: 'config' },
          suggestedAction: 'Use /project to set an active project',
        })],
      };
    }

    const questions: Question[] = [];
    let projectScan: ProjectScan | null = null;

    // Try to load project scan
    try {
      projectScan = await loadProjectScan(project.id);
    } catch {
      debug('Could not load project scan');
    }

    if (!projectScan) {
      questions.push(createQuestion({
        category: 'documentation',
        priority: 'medium',
        question: 'Would you like to run a project scan for deeper analysis?',
        context: 'No project scan data is available. Running a scan will enable architecture-based questions and key file analysis.',
        source: { type: 'project-scan', reference: 'project.json' },
        suggestedAction: 'Run /project scan to generate project analysis',
        requiresScan: true,
      }));
    } else {
      // Analyze project scan for gaps
      questions.push(...this.analyzeProjectScan(projectScan));
    }

    return {
      projectName: project.name,
      projectScanAvailable: !!projectScan,
      scannedAt: projectScan?.scannedAt,
      sourcesAnalyzed: {
        projectScan: !!projectScan,
        readme: !!projectScan?.documentation.readmeSummary,
        issues: { open: 0, analyzed: 0 },  // Will be populated when we add issue scanning
        notes: { total: 0, analyzed: 0 },
      },
      questions: this.prioritizeQuestions(questions),
    };
  }

  /**
   * Analyze a project scan for documentation and architecture gaps
   */
  private analyzeProjectScan(scan: ProjectScan): Question[] {
    const questions: Question[] = [];

    // Check for missing documentation
    if (!scan.documentation.hasDocumentation) {
      questions.push(createQuestion({
        category: 'documentation',
        priority: 'medium',
        question: 'Should documentation be added to this project?',
        context: 'The project scan found no documentation files.',
        source: { type: 'project-scan', reference: 'project.json#documentation' },
        suggestedAction: 'Create a README.md or docs/ directory',
      }));
    }

    // Check for low inline docs coverage
    if (scan.documentation.inlineDocsCoverage === 'none' ||
        scan.documentation.inlineDocsCoverage === 'low') {
      questions.push(createQuestion({
        category: 'documentation',
        priority: 'low',
        question: 'Should inline documentation be improved?',
        context: `Inline documentation coverage is "${scan.documentation.inlineDocsCoverage}".`,
        source: { type: 'project-scan', reference: 'project.json#documentation.inlineDocsCoverage' },
        suggestedAction: 'Add JSDoc/docstrings to key functions and modules',
      }));
    }

    // Check for missing architecture description
    if (!scan.architecture.description || scan.architecture.description.length < 50) {
      questions.push(createQuestion({
        category: 'architecture',
        priority: 'medium',
        question: 'What is the high-level architecture of this project?',
        context: 'The architecture description is missing or very brief.',
        source: { type: 'architecture', reference: 'project.json#architecture' },
        suggestedAction: 'Run a full project scan with LLM analysis to generate architecture description',
      }));
    }

    // Check for key files without documentation
    const undocumentedKeyFiles = scan.keyFiles.filter(f => !f.summary || f.summary.length < 20);
    if (undocumentedKeyFiles.length > 0) {
      const fileList = undocumentedKeyFiles.slice(0, 3).map(f => f.path).join(', ');
      questions.push(createQuestion({
        category: 'documentation',
        priority: 'medium',
        question: `What do these key files do: ${fileList}?`,
        context: `${undocumentedKeyFiles.length} key files lack proper documentation.`,
        source: { type: 'project-scan', reference: 'project.json#keyFiles' },
        suggestedAction: 'Add documentation to key files',
      }));
    }

    // Check for empty overview summary
    if (!scan.overview.summary || scan.overview.summary.length < 20) {
      questions.push(createQuestion({
        category: 'documentation',
        priority: 'high',
        question: 'What does this project do?',
        context: 'The project lacks a clear summary of its purpose.',
        source: { type: 'project-scan', reference: 'project.json#overview.summary' },
        suggestedAction: 'Add a project summary to README or run full scan',
      }));
    }

    return questions;
  }
```

**Step 4: Run test to verify it passes**

Run: `bun run test src/services/gapAnalyzer.test.ts`
Expected: PASS

**Step 5: Commit**

```bash
git add src/services/gapAnalyzer.ts src/services/gapAnalyzer.test.ts
git commit -m "feat(gapAnalyzer): add project context analysis

RED: Wrote tests for analyzeProjectContext with/without project scan
GREEN: Implemented analyzeProjectContext and analyzeProjectScan methods

Part of issue #28"
```

---

## Phase 3: Question Generator Tools

### Task 3.1: Create generate_project_questions Tool

**Files:**
- Create: `src/tools/questionTools.ts`
- Test: `src/tools/questionTools.test.ts`

**Step 1: Write the failing test**

```typescript
// src/tools/questionTools.test.ts
import { describe, it, expect, vi, beforeEach } from 'vitest';
import { generateProjectQuestionsTool } from './questionTools';

vi.mock('../utils/projectConfig', () => ({
  getCurrentProject: vi.fn(),
}));

vi.mock('../services/projectScanBackend', () => ({
  loadProjectScan: vi.fn(),
}));

vi.mock('../services/github', () => ({
  listIssues: vi.fn(),
}));

describe('generateProjectQuestionsTool', () => {
  beforeEach(() => {
    vi.clearAllMocks();
  });

  it('should have correct name and description', () => {
    expect(generateProjectQuestionsTool.inputSchema).toBeDefined();
  });

  it('should return error when no project is set', async () => {
    const { getCurrentProject } = await import('../utils/projectConfig');
    vi.mocked(getCurrentProject).mockResolvedValue(null);

    const result = await generateProjectQuestionsTool.execute({});

    expect(result.success).toBe(false);
    expect(result.error).toContain('No active project');
  });

  it('should return questions for valid project', async () => {
    const { getCurrentProject } = await import('../utils/projectConfig');
    const { loadProjectScan } = await import('../services/projectScanBackend');
    const { listIssues } = await import('../services/github');

    vi.mocked(getCurrentProject).mockResolvedValue({
      id: 'test-project',
      name: 'Test Project',
      path: '/path/to/project',
      github_repo: 'owner/repo',
    });
    vi.mocked(loadProjectScan).mockResolvedValue(null);
    vi.mocked(listIssues).mockResolvedValue([]);

    const result = await generateProjectQuestionsTool.execute({});

    expect(result.success).toBe(true);
    expect(result.questions).toBeDefined();
    expect(Array.isArray(result.questions)).toBe(true);
  });

  it('should filter by category when specified', async () => {
    const { getCurrentProject } = await import('../utils/projectConfig');
    const { loadProjectScan } = await import('../services/projectScanBackend');
    const { listIssues } = await import('../services/github');

    vi.mocked(getCurrentProject).mockResolvedValue({
      id: 'test-project',
      name: 'Test Project',
      path: '/path/to/project',
      github_repo: 'owner/repo',
    });
    vi.mocked(loadProjectScan).mockResolvedValue(null);
    vi.mocked(listIssues).mockResolvedValue([
      {
        number: 1,
        title: 'Test Issue',
        body: 'Short',
        state: 'open',
        labels: [],
        user: { login: 'test' },
        html_url: 'https://github.com/owner/repo/issues/1',
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-01T00:00:00Z',
      },
    ]);

    const result = await generateProjectQuestionsTool.execute({
      categories: ['requirements'],
    });

    expect(result.success).toBe(true);
    // All returned questions should be 'requirements' category
    result.questions.forEach((q: any) => {
      expect(q.category).toBe('requirements');
    });
  });

  it('should filter by min_priority when specified', async () => {
    const { getCurrentProject } = await import('../utils/projectConfig');
    const { loadProjectScan } = await import('../services/projectScanBackend');
    const { listIssues } = await import('../services/github');

    vi.mocked(getCurrentProject).mockResolvedValue({
      id: 'test-project',
      name: 'Test Project',
      path: '/path/to/project',
      github_repo: 'owner/repo',
    });
    vi.mocked(loadProjectScan).mockResolvedValue(null);
    vi.mocked(listIssues).mockResolvedValue([
      {
        number: 1,
        title: 'Test Issue',
        body: 'Short',
        state: 'open',
        labels: [],
        user: { login: 'test' },
        html_url: 'https://github.com/owner/repo/issues/1',
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-01T00:00:00Z',
      },
    ]);

    const result = await generateProjectQuestionsTool.execute({
      min_priority: 'high',
    });

    expect(result.success).toBe(true);
    // All returned questions should be 'high' priority
    result.questions.forEach((q: any) => {
      expect(q.priority).toBe('high');
    });
  });
});
```

**Step 2: Run test to verify it fails**

Run: `bun run test src/tools/questionTools.test.ts`
Expected: FAIL with "Cannot find module './questionTools'"

**Step 3: Write minimal implementation**

```typescript
// src/tools/questionTools.ts
/**
 * Context-Aware Question Generator Tools
 *
 * These tools are exposed to the LLM for generating questions about project context.
 * Each tool's `description` field is a @prompt that instructs the LLM
 * on when and how to use the tool.
 */
import { tool } from './instrumentedTool';
import * as z from 'zod';
import { getCurrentProject } from '../utils/projectConfig';
import { loadProjectScan } from '../services/projectScanBackend';
import { listIssues } from '../services/github';
import { GapAnalyzer } from '../services/gapAnalyzer';
import type { Question, QuestionCategory, QuestionPriority } from '../types/questions';
import { debug } from '../utils/logger';

const PRIORITY_LEVELS: Record<QuestionPriority, number> = {
  high: 0,
  medium: 1,
  low: 2,
};

/**
 * @prompt Tool: generate_project_questions
 * Description and parameter descriptions sent to LLM explaining tool usage.
 */
export const generateProjectQuestionsTool = tool({
  description: `Analyze the current project's context and generate prioritized questions to surface information gaps.

This tool scans:
- Open GitHub issues for missing acceptance criteria, vague descriptions, missing labels
- Project scan data (if available) for documentation gaps and architecture questions
- Project notes for inconsistencies

Use this when:
- User asks "what questions should I be asking?"
- User asks "what am I missing?"
- User wants to identify gaps in their project documentation or issues

Returns questions grouped by category and sorted by priority.`,

  inputSchema: z.object({
    categories: z.array(z.enum([
      'requirements', 'technical', 'documentation', 'process', 'consistency', 'architecture'
    ])).optional().describe('Filter questions to specific categories'),
    min_priority: z.enum(['high', 'medium', 'low']).optional().describe('Minimum priority level to include'),
    max_questions: z.number().optional().default(20).describe('Maximum number of questions to return'),
    include_issues: z.boolean().optional().default(true).describe('Whether to analyze GitHub issues'),
  }),

  execute: async ({ categories, min_priority, max_questions = 20, include_issues = true }) => {
    debug('generate_project_questions called with:', { categories, min_priority, max_questions });

    try {
      const project = await getCurrentProject();

      if (!project) {
        return {
          success: false,
          error: 'No active project set. Use /project to set an active project first.',
          questions: [],
        };
      }

      const analyzer = new GapAnalyzer();
      let allQuestions: Question[] = [];

      // Analyze project context (includes project scan if available)
      const projectAnalysis = await analyzer.analyzeProjectContext();
      allQuestions.push(...projectAnalysis.questions);

      // Analyze GitHub issues if requested and repo is configured
      if (include_issues && project.github_repo) {
        try {
          const [owner, repo] = project.github_repo.split('/');
          if (owner && repo) {
            const issues = await listIssues(owner, repo, { state: 'open', per_page: 30 });

            for (const issue of issues) {
              const issueAnalysis = await analyzer.analyzeIssue(issue, []);
              allQuestions.push(...issueAnalysis.questions);
            }
          }
        } catch (error) {
          debug('Error analyzing GitHub issues:', error);
          // Continue without issue analysis
        }
      }

      // Filter by categories if specified
      if (categories && categories.length > 0) {
        allQuestions = allQuestions.filter(q => categories.includes(q.category));
      }

      // Filter by minimum priority if specified
      if (min_priority) {
        const minLevel = PRIORITY_LEVELS[min_priority];
        allQuestions = allQuestions.filter(q => PRIORITY_LEVELS[q.priority] <= minLevel);
      }

      // Prioritize and limit
      const prioritizedQuestions = analyzer.prioritizeQuestions(allQuestions).slice(0, max_questions);

      // Build summary
      const summary = {
        totalQuestions: prioritizedQuestions.length,
        byCategory: {} as Record<string, number>,
        byPriority: {} as Record<string, number>,
      };

      for (const q of prioritizedQuestions) {
        summary.byCategory[q.category] = (summary.byCategory[q.category] || 0) + 1;
        summary.byPriority[q.priority] = (summary.byPriority[q.priority] || 0) + 1;
      }

      return {
        success: true,
        project: project.name,
        scannedAt: new Date().toISOString(),
        projectScanAvailable: projectAnalysis.projectScanAvailable,
        sourcesAnalyzed: projectAnalysis.sourcesAnalyzed,
        questions: prioritizedQuestions,
        summary,
        message: formatQuestionsMessage(prioritizedQuestions, summary, projectAnalysis.projectScanAvailable),
      };

    } catch (error) {
      debug('Error in generate_project_questions:', error);
      return {
        success: false,
        error: `Failed to generate questions: ${error instanceof Error ? error.message : String(error)}`,
        questions: [],
      };
    }
  },
});

/**
 * Format questions into a readable message
 */
function formatQuestionsMessage(
  questions: Question[],
  summary: { totalQuestions: number; byCategory: Record<string, number>; byPriority: Record<string, number> },
  projectScanAvailable: boolean
): string {
  const lines: string[] = [];

  lines.push(`## Generated ${summary.totalQuestions} Question${summary.totalQuestions === 1 ? '' : 's'}`);
  lines.push('');

  if (!projectScanAvailable) {
    lines.push('> **Note:** Run `/project scan` for deeper architecture and documentation analysis.');
    lines.push('');
  }

  // Group by priority
  const high = questions.filter(q => q.priority === 'high');
  const medium = questions.filter(q => q.priority === 'medium');
  const low = questions.filter(q => q.priority === 'low');

  if (high.length > 0) {
    lines.push('### High Priority');
    high.forEach((q, i) => {
      lines.push(`${i + 1}. **${q.question}**`);
      lines.push(`   - ${q.context}`);
      lines.push(`   - Source: ${q.source.reference}`);
      if (q.suggestedAction) {
        lines.push(`   - Action: ${q.suggestedAction}`);
      }
    });
    lines.push('');
  }

  if (medium.length > 0) {
    lines.push('### Medium Priority');
    medium.forEach((q, i) => {
      lines.push(`${i + 1}. **${q.question}**`);
      lines.push(`   - ${q.context}`);
      lines.push(`   - Source: ${q.source.reference}`);
    });
    lines.push('');
  }

  if (low.length > 0) {
    lines.push('### Low Priority');
    low.forEach((q, i) => {
      lines.push(`${i + 1}. ${q.question}`);
      lines.push(`   - Source: ${q.source.reference}`);
    });
    lines.push('');
  }

  // Summary by category
  lines.push('### Summary by Category');
  Object.entries(summary.byCategory).forEach(([cat, count]) => {
    lines.push(`- ${cat}: ${count}`);
  });

  return lines.join('\n');
}
```

**Step 4: Run test to verify it passes**

Run: `bun run test src/tools/questionTools.test.ts`
Expected: PASS

**Step 5: Commit**

```bash
git add src/tools/questionTools.ts src/tools/questionTools.test.ts
git commit -m "feat(questionTools): add generate_project_questions tool

RED: Wrote tests for generate_project_questions with filtering
GREEN: Implemented tool with project context and issue analysis

Part of issue #28"
```

---

### Task 3.2: Create generate_issue_questions Tool

**Files:**
- Modify: `src/tools/questionTools.ts`
- Test: `src/tools/questionTools.test.ts`

**Step 1: Write the failing test**

```typescript
// Add to src/tools/questionTools.test.ts

describe('generateIssueQuestionsTool', () => {
  beforeEach(() => {
    vi.clearAllMocks();
  });

  it('should have correct name and description', async () => {
    const { generateIssueQuestionsTool } = await import('./questionTools');
    expect(generateIssueQuestionsTool.inputSchema).toBeDefined();
  });

  it('should return error when no project is set', async () => {
    const { generateIssueQuestionsTool } = await import('./questionTools');
    const { getCurrentProject } = await import('../utils/projectConfig');
    vi.mocked(getCurrentProject).mockResolvedValue(null);

    const result = await generateIssueQuestionsTool.execute({ issue_number: 42 });

    expect(result.success).toBe(false);
    expect(result.error).toContain('No active project');
  });

  it('should return questions for a specific issue', async () => {
    const { generateIssueQuestionsTool } = await import('./questionTools');
    const { getCurrentProject } = await import('../utils/projectConfig');
    const { getIssueWithComments } = await import('../services/github');

    vi.mocked(getCurrentProject).mockResolvedValue({
      id: 'test-project',
      name: 'Test Project',
      path: '/path/to/project',
      github_repo: 'owner/repo',
    });

    vi.mock('../services/github', async (importOriginal) => {
      const original = await importOriginal() as any;
      return {
        ...original,
        getIssueWithComments: vi.fn().mockResolvedValue({
          issue: {
            number: 42,
            title: 'Test Issue',
            body: 'Short description',
            state: 'open',
            labels: [],
            user: { login: 'test' },
            html_url: 'https://github.com/owner/repo/issues/42',
            created_at: '2025-01-01T00:00:00Z',
            updated_at: '2025-01-01T00:00:00Z',
          },
          comments: [],
          pagination: { page: 1, per_page: 100, total: 0, has_next_page: false },
        }),
      };
    });

    const result = await generateIssueQuestionsTool.execute({ issue_number: 42 });

    expect(result.success).toBe(true);
    expect(result.issueNumber).toBe(42);
    expect(result.questions).toBeDefined();
    expect(result.questions.length).toBeGreaterThan(0);
  });
});
```

**Step 2: Run test to verify it fails**

Run: `bun run test src/tools/questionTools.test.ts`
Expected: FAIL with "generateIssueQuestionsTool is not defined"

**Step 3: Add implementation**

```typescript
// Add to src/tools/questionTools.ts

import { getIssueWithComments } from '../services/github';

/**
 * @prompt Tool: generate_issue_questions
 * Description and parameter descriptions sent to LLM explaining tool usage.
 */
export const generateIssueQuestionsTool = tool({
  description: `Analyze a specific GitHub issue and generate questions to identify gaps or clarify requirements.

This tool examines:
- Issue description completeness
- Presence of acceptance criteria
- Label and assignee status
- Comment thread for context

Use this when:
- User asks "are there any gaps in issue #X?"
- User wants to review an issue before starting work
- User needs to understand what's unclear about an issue

Returns targeted questions specific to the issue.`,

  inputSchema: z.object({
    issue_number: z.number().describe('The GitHub issue number to analyze'),
  }),

  execute: async ({ issue_number }) => {
    debug('generate_issue_questions called for issue:', issue_number);

    try {
      const project = await getCurrentProject();

      if (!project) {
        return {
          success: false,
          error: 'No active project set. Use /project to set an active project first.',
          issueNumber: issue_number,
          issueTitle: '',
          questions: [],
        };
      }

      if (!project.github_repo) {
        return {
          success: false,
          error: 'Current project does not have a GitHub repository configured.',
          issueNumber: issue_number,
          issueTitle: '',
          questions: [],
        };
      }

      const [owner, repo] = project.github_repo.split('/');
      if (!owner || !repo) {
        return {
          success: false,
          error: `Invalid GitHub repository format: ${project.github_repo}`,
          issueNumber: issue_number,
          issueTitle: '',
          questions: [],
        };
      }

      // Fetch the issue with comments
      const { issue, comments } = await getIssueWithComments(owner, repo, issue_number, {
        includeComments: true,
        commentsPerPage: 100,
      });

      const analyzer = new GapAnalyzer();
      const analysis = await analyzer.analyzeIssue(issue, comments);

      // Build summary
      const summary = {
        totalQuestions: analysis.questions.length,
        byCategory: {} as Record<string, number>,
      };

      for (const q of analysis.questions) {
        summary.byCategory[q.category] = (summary.byCategory[q.category] || 0) + 1;
      }

      return {
        success: true,
        issueNumber: issue.number,
        issueTitle: issue.title,
        issueUrl: issue.html_url,
        questions: analysis.questions,
        summary,
        message: formatIssueQuestionsMessage(issue, analysis.questions),
      };

    } catch (error) {
      debug('Error in generate_issue_questions:', error);
      return {
        success: false,
        error: `Failed to analyze issue: ${error instanceof Error ? error.message : String(error)}`,
        issueNumber: issue_number,
        issueTitle: '',
        questions: [],
      };
    }
  },
});

/**
 * Format issue questions into a readable message
 */
function formatIssueQuestionsMessage(issue: any, questions: Question[]): string {
  const lines: string[] = [];

  lines.push(`## Analysis of Issue #${issue.number}: ${issue.title}`);
  lines.push('');

  if (questions.length === 0) {
    lines.push('This issue appears to be well-documented with no obvious gaps.');
    return lines.join('\n');
  }

  lines.push(`Found ${questions.length} potential gap${questions.length === 1 ? '' : 's'}:`);
  lines.push('');

  questions.forEach((q, i) => {
    const priorityEmoji = q.priority === 'high' ? 'ðŸ”´' : q.priority === 'medium' ? 'ðŸŸ¡' : 'ðŸŸ¢';
    lines.push(`${i + 1}. ${priorityEmoji} **${q.question}**`);
    lines.push(`   - ${q.context}`);
    if (q.suggestedAction) {
      lines.push(`   - ðŸ’¡ ${q.suggestedAction}`);
    }
    lines.push('');
  });

  return lines.join('\n');
}
```

**Step 4: Run test to verify it passes**

Run: `bun run test src/tools/questionTools.test.ts`
Expected: PASS

**Step 5: Commit**

```bash
git add src/tools/questionTools.ts src/tools/questionTools.test.ts
git commit -m "feat(questionTools): add generate_issue_questions tool

RED: Wrote tests for generate_issue_questions with mocked GitHub API
GREEN: Implemented tool with issue-specific gap analysis

Part of issue #28"
```

---

### Task 3.3: Create identify_information_gaps and suggest_clarifications Tools

**Files:**
- Modify: `src/tools/questionTools.ts`
- Test: `src/tools/questionTools.test.ts`

**Step 1: Write the failing test**

```typescript
// Add to src/tools/questionTools.test.ts

describe('identifyInformationGapsTool', () => {
  it('should analyze README for gaps', async () => {
    const { identifyInformationGapsTool } = await import('./questionTools');
    const { getCurrentProject } = await import('../utils/projectConfig');

    vi.mocked(getCurrentProject).mockResolvedValue({
      id: 'test-project',
      name: 'Test Project',
      path: '/tmp/test-project',
      github_repo: 'owner/repo',
    });

    // Mock file reading
    vi.mock('fs/promises', () => ({
      readFile: vi.fn().mockResolvedValue('# Project\n\nShort readme.'),
    }));

    const result = await identifyInformationGapsTool.execute({
      target: 'readme',
    });

    expect(result.success).toBe(true);
    expect(result.target).toBe('readme');
  });
});

describe('suggestClarificationsTool', () => {
  it('should suggest clarifications for draft issue', async () => {
    const { suggestClarificationsTool } = await import('./questionTools');

    const result = await suggestClarificationsTool.execute({
      draft_type: 'issue',
      content: 'Fix the bug',
    });

    expect(result.success).toBe(true);
    expect(result.clarifications.length).toBeGreaterThan(0);
    expect(result.clarifications.some((c: any) =>
      c.question.toLowerCase().includes('detail') ||
      c.question.toLowerCase().includes('criteria')
    )).toBe(true);
  });
});
```

**Step 2: Run test to verify it fails**

Run: `bun run test src/tools/questionTools.test.ts`
Expected: FAIL with "identifyInformationGapsTool is not defined"

**Step 3: Add implementation**

```typescript
// Add to src/tools/questionTools.ts

import { readFile } from 'fs/promises';
import { join } from 'path';
import { existsSync } from 'fs';

/**
 * @prompt Tool: identify_information_gaps
 * Description and parameter descriptions sent to LLM explaining tool usage.
 */
export const identifyInformationGapsTool = tool({
  description: `Scan a specific target (README, documentation, codebase) for information gaps.

Use this when:
- User asks "what's missing from our documentation?"
- User asks "what's unclear about the README?"
- User wants to identify gaps in a specific area

Returns questions about missing or unclear information in the target.`,

  inputSchema: z.object({
    target: z.enum(['readme', 'documentation', 'codebase']).describe('What to analyze for gaps'),
  }),

  execute: async ({ target }) => {
    debug('identify_information_gaps called for target:', target);

    try {
      const project = await getCurrentProject();

      if (!project) {
        return {
          success: false,
          error: 'No active project set.',
          target,
          gaps: [],
        };
      }

      const questions: Question[] = [];

      if (target === 'readme') {
        const readmePath = join(project.path, 'README.md');

        if (!existsSync(readmePath)) {
          questions.push(createQuestion({
            category: 'documentation',
            priority: 'high',
            question: 'Should this project have a README?',
            context: 'No README.md file found in the project root.',
            source: { type: 'readme', reference: 'README.md' },
            suggestedAction: 'Create a README.md with project overview, setup instructions, and usage examples',
          }));
        } else {
          try {
            const content = await readFile(readmePath, 'utf-8');
            questions.push(...analyzeReadme(content));
          } catch {
            questions.push(createQuestion({
              category: 'documentation',
              priority: 'medium',
              question: 'Could not read README - is it accessible?',
              context: 'Failed to read README.md file.',
              source: { type: 'readme', reference: 'README.md' },
            }));
          }
        }
      } else if (target === 'documentation') {
        // Check for docs directory
        const docsPath = join(project.path, 'docs');
        if (!existsSync(docsPath)) {
          questions.push(createQuestion({
            category: 'documentation',
            priority: 'medium',
            question: 'Should this project have a docs/ directory?',
            context: 'No dedicated documentation directory found.',
            source: { type: 'file', reference: 'docs/' },
            suggestedAction: 'Create a docs/ directory with API reference, guides, or architecture docs',
          }));
        }

        // Leverage project scan if available
        const scan = await loadProjectScan(project.id);
        if (scan) {
          if (!scan.documentation.hasDocumentation) {
            questions.push(createQuestion({
              category: 'documentation',
              priority: 'medium',
              question: 'What documentation does this project need?',
              context: 'Project scan found no documentation files.',
              source: { type: 'project-scan', reference: 'project.json#documentation' },
            }));
          }
        }
      } else if (target === 'codebase') {
        const scan = await loadProjectScan(project.id);
        if (!scan) {
          questions.push(createQuestion({
            category: 'documentation',
            priority: 'medium',
            question: 'Run a project scan for codebase analysis?',
            context: 'No project scan available. Run /project scan first.',
            source: { type: 'project-scan', reference: 'project.json' },
            suggestedAction: 'Run /project scan to analyze codebase',
            requiresScan: true,
          }));
        } else {
          // Analyze inline docs coverage
          if (scan.documentation.inlineDocsCoverage === 'none') {
            questions.push(createQuestion({
              category: 'documentation',
              priority: 'medium',
              question: 'Should inline documentation be added?',
              context: 'No inline documentation (comments/docstrings) found.',
              source: { type: 'project-scan', reference: 'project.json#documentation.inlineDocsCoverage' },
              suggestedAction: 'Add JSDoc/docstrings to exported functions',
            }));
          }

          // Check key files for missing summaries
          const undocumented = scan.keyFiles.filter(f => !f.summary);
          if (undocumented.length > 0) {
            questions.push(createQuestion({
              category: 'documentation',
              priority: 'low',
              question: `What do these ${undocumented.length} key files do?`,
              context: `Key files without descriptions: ${undocumented.slice(0, 5).map(f => f.path).join(', ')}`,
              source: { type: 'project-scan', reference: 'project.json#keyFiles' },
            }));
          }
        }
      }

      const summary = {
        totalGaps: questions.length,
        byCategory: {} as Record<string, number>,
      };

      for (const q of questions) {
        summary.byCategory[q.category] = (summary.byCategory[q.category] || 0) + 1;
      }

      return {
        success: true,
        target,
        gaps: questions,
        summary,
        message: `Found ${questions.length} potential gap${questions.length === 1 ? '' : 's'} in ${target}`,
      };

    } catch (error) {
      debug('Error in identify_information_gaps:', error);
      return {
        success: false,
        error: `Failed to identify gaps: ${error instanceof Error ? error.message : String(error)}`,
        target,
        gaps: [],
      };
    }
  },
});

/**
 * Analyze README content for common gaps
 */
function analyzeReadme(content: string): Question[] {
  const questions: Question[] = [];
  const lowerContent = content.toLowerCase();

  // Check for installation/setup section
  if (!lowerContent.includes('install') && !lowerContent.includes('setup') && !lowerContent.includes('getting started')) {
    questions.push(createQuestion({
      category: 'documentation',
      priority: 'high',
      question: 'How do users install or set up this project?',
      context: 'README lacks installation or setup instructions.',
      source: { type: 'readme', reference: 'README.md' },
      suggestedAction: 'Add an Installation or Getting Started section',
    }));
  }

  // Check for usage examples
  if (!lowerContent.includes('usage') && !lowerContent.includes('example') && !content.includes('```')) {
    questions.push(createQuestion({
      category: 'documentation',
      priority: 'medium',
      question: 'How do users use this project?',
      context: 'README lacks usage examples or code snippets.',
      source: { type: 'readme', reference: 'README.md' },
      suggestedAction: 'Add usage examples with code blocks',
    }));
  }

  // Check for very short README
  if (content.length < 500) {
    questions.push(createQuestion({
      category: 'documentation',
      priority: 'medium',
      question: 'Is the README complete?',
      context: 'README is quite short and may be missing important information.',
      source: { type: 'readme', reference: 'README.md' },
      suggestedAction: 'Expand README with project overview, features, and contribution guidelines',
    }));
  }

  // Check for license info
  if (!lowerContent.includes('license')) {
    questions.push(createQuestion({
      category: 'documentation',
      priority: 'low',
      question: 'What license is this project under?',
      context: 'README does not mention licensing.',
      source: { type: 'readme', reference: 'README.md' },
      suggestedAction: 'Add a License section or reference LICENSE file',
    }));
  }

  return questions;
}

/**
 * @prompt Tool: suggest_clarifications
 * Description and parameter descriptions sent to LLM explaining tool usage.
 */
export const suggestClarificationsTool = tool({
  description: `Analyze a draft (issue, PR, document) and suggest clarifications before submission.

Use this when:
- User says "help me improve this issue before I submit it"
- User asks "what's missing from this?"
- User wants feedback on a draft before posting

Returns suggestions for what to clarify or add.`,

  inputSchema: z.object({
    draft_type: z.enum(['issue', 'pr', 'note', 'document']).describe('Type of draft being reviewed'),
    content: z.string().describe('The draft content to analyze'),
    title: z.string().optional().describe('Optional title of the draft'),
  }),

  execute: async ({ draft_type, content, title }) => {
    debug('suggest_clarifications called for:', { draft_type, title });

    const questions: Question[] = [];

    // Analyze based on draft type
    if (draft_type === 'issue') {
      // Check for acceptance criteria
      if (!content.toLowerCase().includes('acceptance criteria') &&
          !content.toLowerCase().includes('done when') &&
          !/\[\s*[x ]\s*\]/.test(content)) {
        questions.push(createQuestion({
          category: 'requirements',
          priority: 'high',
          question: 'What are the acceptance criteria for this issue?',
          context: 'Issue draft lacks acceptance criteria.',
          source: { type: 'issue', reference: 'draft' },
          suggestedAction: 'Add an "Acceptance Criteria" section with checkboxes',
        }));
      }

      // Check for short content
      if (content.length < 100) {
        questions.push(createQuestion({
          category: 'requirements',
          priority: 'high',
          question: 'Can you provide more detail about this issue?',
          context: 'The description is very brief.',
          source: { type: 'issue', reference: 'draft' },
          suggestedAction: 'Add problem description, expected behavior, and context',
        }));
      }

      // Check for bug-related issues missing reproduction steps
      if ((title?.toLowerCase().includes('bug') || content.toLowerCase().includes('bug')) &&
          !content.toLowerCase().includes('reproduce') &&
          !content.toLowerCase().includes('steps')) {
        questions.push(createQuestion({
          category: 'requirements',
          priority: 'medium',
          question: 'How can this bug be reproduced?',
          context: 'Bug report lacks reproduction steps.',
          source: { type: 'issue', reference: 'draft' },
          suggestedAction: 'Add "Steps to Reproduce" section',
        }));
      }
    } else if (draft_type === 'pr') {
      // Check for linked issue
      if (!content.match(/#\d+/) && !content.toLowerCase().includes('fixes') &&
          !content.toLowerCase().includes('closes')) {
        questions.push(createQuestion({
          category: 'process',
          priority: 'medium',
          question: 'Which issue does this PR address?',
          context: 'PR draft does not reference any issues.',
          source: { type: 'pr', reference: 'draft' },
          suggestedAction: 'Add "Fixes #123" or link to related issue',
        }));
      }

      // Check for testing info
      if (!content.toLowerCase().includes('test')) {
        questions.push(createQuestion({
          category: 'technical',
          priority: 'medium',
          question: 'How was this change tested?',
          context: 'PR draft does not mention testing.',
          source: { type: 'pr', reference: 'draft' },
          suggestedAction: 'Add a "Testing" section describing how changes were verified',
        }));
      }
    }

    const summary = {
      totalClarifications: questions.length,
      byCategory: {} as Record<string, number>,
    };

    for (const q of questions) {
      summary.byCategory[q.category] = (summary.byCategory[q.category] || 0) + 1;
    }

    return {
      success: true,
      draftType: draft_type,
      clarifications: questions,
      summary,
      message: questions.length === 0
        ? 'Draft looks good! No obvious gaps found.'
        : `Found ${questions.length} suggestion${questions.length === 1 ? '' : 's'} for improvement.`,
    };
  },
});

// Import createQuestion at the top of the file
import { createQuestion } from '../types/questions';
```

**Step 4: Run test to verify it passes**

Run: `bun run test src/tools/questionTools.test.ts`
Expected: PASS

**Step 5: Commit**

```bash
git add src/tools/questionTools.ts src/tools/questionTools.test.ts
git commit -m "feat(questionTools): add identify_information_gaps and suggest_clarifications

RED: Wrote tests for gap identification and draft clarification tools
GREEN: Implemented tools with README analysis and draft review

Part of issue #28"
```

---

### Task 3.4: Register Question Tools

**Files:**
- Modify: `src/tools/registry.ts`
- Test: Build verification

**Step 1: Add imports and registration**

```typescript
// Add to src/tools/registry.ts imports
import {
  generateProjectQuestionsTool,
  generateIssueQuestionsTool,
  identifyInformationGapsTool,
  suggestClarificationsTool,
} from './questionTools';

// Add to toolRegistry object
const toolRegistry: ToolRegistry = {
  // ... existing tools ...

  // Context-Aware Questions
  generate_project_questions: generateProjectQuestionsTool,
  generate_issue_questions: generateIssueQuestionsTool,
  identify_information_gaps: identifyInformationGapsTool,
  suggest_clarifications: suggestClarificationsTool,
};
```

**Step 2: Verify build**

Run: `bun run build`
Expected: Build succeeds

**Step 3: Run all tests**

Run: `bun run test`
Expected: All tests pass

**Step 4: Commit**

```bash
git add src/tools/registry.ts
git commit -m "feat(registry): register context-aware question tools

Added generate_project_questions, generate_issue_questions,
identify_information_gaps, and suggest_clarifications to tool registry

Part of issue #28"
```

---

## Phase 4: Skill Creation

### Task 4.1: Create context-aware-questions Skill

**Files:**
- Create: `skills/context-aware-questions/SKILL.md`
- Test: Manual verification via `/skills list`

**Step 1: Create the skill file**

```markdown
---
name: context-aware-questions
description: "Proactively identifies information gaps in project context and generates prioritized, actionable questions. Scans issues, PRs, codebase, notes, and project scan data to surface missing requirements, ambiguities, and inconsistencies."
---

# Context-Aware Question Generator

Identify what's missing, unclear, or inconsistent in your project. Generate targeted questions to fill information gaps before they become problems.

## When to Use This Skill

Activate when the user:
- Asks "what questions should I be asking?"
- Asks "what am I missing?"
- Asks "what's unclear?"
- Wants help thinking through a problem
- Asks to review an issue for completeness
- Asks about documentation gaps
- Asks to clarify something before submission

## Available Tools

| Tool | Use When |
|------|----------|
| `generate_project_questions` | User wants overall project analysis for gaps |
| `generate_issue_questions` | User asks about a specific issue (#X) |
| `identify_information_gaps` | User asks about README, docs, or codebase gaps |
| `suggest_clarifications` | User has a draft (issue, PR) to review |

## Question Categories

Questions are categorized to help prioritize:

- **Requirements**: Missing acceptance criteria, vague descriptions
- **Technical**: Undocumented code, unclear error handling
- **Documentation**: Missing docs, outdated README
- **Process**: Stale issues, missing assignees
- **Consistency**: Contradictory information
- **Architecture**: Component relationships, dependency issues

## Priority Levels

- **High**: Blocking issues, missing critical info
- **Medium**: Should be addressed soon
- **Low**: Nice to have, minor improvements

## Example Conversations

**User:** "What questions should I be asking about this project?"
â†’ Use `generate_project_questions` to analyze overall project context

**User:** "Are there any gaps in issue #42?"
â†’ Use `generate_issue_questions` with issue_number=42

**User:** "What's missing from our README?"
â†’ Use `identify_information_gaps` with target="readme"

**User:** "Review this issue draft before I submit it"
â†’ Use `suggest_clarifications` with the draft content

**User:** "What am I missing here?"
â†’ Use `generate_project_questions` for broad analysis

## Tips

1. **Run project scan first** for deeper architecture questions: `/project scan`
2. **Filter by category** if you want focused questions
3. **Set min_priority** to "high" if you only want critical gaps
4. **Use issue analysis** before starting work on any issue
5. **Review drafts** before submitting issues or PRs

## Integration with Project Scan

When `project.json` exists (from `/project scan`), questions leverage:
- Architecture component analysis
- Key file documentation status
- Dependency information
- Overall project summary

Without a scan, basic questions are generated with a suggestion to run the scan.
```

**Step 2: Verify skill is discovered**

Run: `bun run start` then `/skills list`
Expected: `context-aware-questions` appears in the list

**Step 3: Commit**

```bash
git add skills/context-aware-questions/SKILL.md
git commit -m "feat(skills): add context-aware-questions skill

Created skill definition with:
- Triggers for natural language activation
- Tool mapping documentation
- Category and priority explanations
- Example conversations

Part of issue #28"
```

---

## Phase 5: Integration Testing

### Task 5.1: Add Integration Tests

**Files:**
- Create: `src/tools/questionTools.integration.test.ts`

**Step 1: Write integration test**

```typescript
// src/tools/questionTools.integration.test.ts
import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import {
  generateProjectQuestionsTool,
  generateIssueQuestionsTool,
  identifyInformationGapsTool,
  suggestClarificationsTool,
} from './questionTools';

/**
 * Integration tests for question tools
 * These test the full flow without external dependencies
 */
describe('Question Tools Integration', () => {
  beforeEach(() => {
    vi.clearAllMocks();
  });

  describe('Tool Schema Validation', () => {
    it('generate_project_questions has valid schema', () => {
      expect(generateProjectQuestionsTool.inputSchema).toBeDefined();
      // Verify it can parse valid input
      const result = generateProjectQuestionsTool.inputSchema.safeParse({
        categories: ['requirements'],
        min_priority: 'high',
        max_questions: 10,
      });
      expect(result.success).toBe(true);
    });

    it('generate_issue_questions has valid schema', () => {
      expect(generateIssueQuestionsTool.inputSchema).toBeDefined();
      const result = generateIssueQuestionsTool.inputSchema.safeParse({
        issue_number: 42,
      });
      expect(result.success).toBe(true);
    });

    it('identify_information_gaps has valid schema', () => {
      expect(identifyInformationGapsTool.inputSchema).toBeDefined();
      const result = identifyInformationGapsTool.inputSchema.safeParse({
        target: 'readme',
      });
      expect(result.success).toBe(true);
    });

    it('suggest_clarifications has valid schema', () => {
      expect(suggestClarificationsTool.inputSchema).toBeDefined();
      const result = suggestClarificationsTool.inputSchema.safeParse({
        draft_type: 'issue',
        content: 'Fix the bug',
      });
      expect(result.success).toBe(true);
    });
  });

  describe('Error Handling', () => {
    it('tools handle missing project gracefully', async () => {
      vi.mock('../utils/projectConfig', () => ({
        getCurrentProject: vi.fn().mockResolvedValue(null),
      }));

      const projectResult = await generateProjectQuestionsTool.execute({});
      expect(projectResult.success).toBe(false);
      expect(projectResult.error).toBeDefined();

      const issueResult = await generateIssueQuestionsTool.execute({ issue_number: 1 });
      expect(issueResult.success).toBe(false);
      expect(issueResult.error).toBeDefined();
    });
  });

  describe('Draft Analysis', () => {
    it('detects missing acceptance criteria in issue draft', async () => {
      const result = await suggestClarificationsTool.execute({
        draft_type: 'issue',
        content: 'We should fix the login bug',
        title: 'Login bug',
      });

      expect(result.success).toBe(true);
      expect(result.clarifications.length).toBeGreaterThan(0);
      expect(result.clarifications.some(c =>
        c.question.toLowerCase().includes('acceptance criteria') ||
        c.question.toLowerCase().includes('detail')
      )).toBe(true);
    });

    it('detects missing reproduction steps in bug report', async () => {
      const result = await suggestClarificationsTool.execute({
        draft_type: 'issue',
        content: 'There is a bug when clicking the button. It crashes.',
        title: 'Bug: Button crash',
      });

      expect(result.success).toBe(true);
      expect(result.clarifications.some(c =>
        c.question.toLowerCase().includes('reproduce')
      )).toBe(true);
    });

    it('detects missing linked issue in PR', async () => {
      const result = await suggestClarificationsTool.execute({
        draft_type: 'pr',
        content: 'Added new feature for user profiles.',
        title: 'Add user profiles',
      });

      expect(result.success).toBe(true);
      expect(result.clarifications.some(c =>
        c.question.toLowerCase().includes('issue')
      )).toBe(true);
    });

    it('approves well-formed issue draft', async () => {
      const result = await suggestClarificationsTool.execute({
        draft_type: 'issue',
        content: `## Problem
The login form does not validate email format before submission.

## Expected Behavior
Email should be validated client-side with appropriate error messages.

## Acceptance Criteria
- [ ] Email format is validated on blur
- [ ] Error message shows for invalid emails
- [ ] Form cannot submit with invalid email

## Steps to Reproduce
1. Go to /login
2. Enter "invalid-email" in email field
3. Click submit
4. Observe no validation error`,
        title: 'Bug: Missing email validation on login form',
      });

      expect(result.success).toBe(true);
      // Well-formed issue should have fewer clarifications
      expect(result.clarifications.length).toBeLessThan(3);
    });
  });
});
```

**Step 2: Run integration tests**

Run: `bun run test src/tools/questionTools.integration.test.ts`
Expected: PASS

**Step 3: Commit**

```bash
git add src/tools/questionTools.integration.test.ts
git commit -m "test(questionTools): add integration tests

Tests tool schema validation, error handling, and draft analysis

Part of issue #28"
```

---

## Phase 6: Final Steps

### Task 6.1: Update Exports and Documentation

**Files:**
- Modify: `src/types/index.ts` (if exists) or create export file
- Verify: Build and lint

**Step 1: Export types**

```typescript
// If src/types/index.ts exists, add:
export * from './questions';

// Or create src/types/questions.ts exports in the main types
```

**Step 2: Run build and lint**

Run: `bun run build && bun run lint && bun run typecheck`
Expected: All pass

**Step 3: Run full test suite**

Run: `bun run test --coverage`
Expected: All tests pass, coverage meets threshold

**Step 4: Final commit**

```bash
git add -A
git commit -m "feat(context-aware-questions): complete implementation

Implements issue #28: Context-Aware Question Generator

New Features:
- Question types with categories and priorities
- GapAnalyzer service for issue and project analysis
- Four AI tools: generate_project_questions, generate_issue_questions,
  identify_information_gaps, suggest_clarifications
- context-aware-questions skill for natural language triggers

Integration:
- Leverages project scan data when available
- Analyzes GitHub issues for missing acceptance criteria, labels
- Reviews drafts before submission
- Prioritizes questions by impact

Closes #28"
```

---

## Verification Checklist

Before marking complete:

- [ ] All tests pass: `bun run test`
- [ ] Build succeeds: `bun run build`
- [ ] Lint passes: `bun run lint`
- [ ] Type check passes: `bun run typecheck`
- [ ] Coverage meets threshold: `bun run test --coverage`
- [ ] Skill appears in `/skills list`
- [ ] Tools appear in tool registry
- [ ] Manual test: Ask "what questions should I be asking?" and verify response

---

**Plan complete and saved to `docs/plans/2026-01-06-context-aware-questions.md`. Two execution options:**

**1. Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration

**2. Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints

**Which approach?**
